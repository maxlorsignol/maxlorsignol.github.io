[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Lorsignol",
    "section": "",
    "text": "Hi, my name is Max Lorsignol.\nI am currently pursuing a Master’s degree in Geomatics for Environmental Management at UBC. What is geomatics, you ask? In a nutshell, geomatics is the science and technology of collecting, analyzing, managing, and visualizing spatial data. Ever since I was a child, I’ve been fascinated by the maps my father used for his mountaineering adventures. Today, I’m leveraging that lifelong love for mapping and applying it with modern tools and standards to be a part of the conservation and environmental efforts of the 21st century.\nWhen I’m not crafting digital maps or doing spatial analyses, you’ll find me outdoors, navigating the rugged mountains of the west coast with the same passion for exploration. My goal is to combine the skills I’ve developed through my education and personal experiences to address pressing issues related to environmental management, natural resources, climate change, and land use management. I hope to make a meaningful difference in creating sustainable solutions for the challenges our world faces today.\n\n\n\nGIF of a forest plot with Individual Trees Segmented using li2012, colorized by TreeID"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Education\n\nB.Sc Earth and Environmental Science UBC Okanagan\nMasters in Geomatics for Environmental Management UBC Vancouver\n\n\n\nProfessional Experience\n\nLead Field Technician at AquaCoustic Remote Technologies Inc\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software\n\nVisitors Service Attendant for Banff National Park\n\nPublic outreach - Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively\n\n\n\n\nPublications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Peer-reviewed publications:\nDespite listing them on your resume, it may also be pertinent to create a seperate tab for publications and reports. As your career progresses, this list may become quite long, so be sure to organize things. You may want to point to ‘most recent’ publications, or categorize things by project/topic."
  },
  {
    "objectID": "content_development.html",
    "href": "content_development.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Project Deliverable 1\nThis is a sample page where you can archive project deliverables."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "This project focuses on identifying and designating Other Effective Area-Based Conservation Measures (OECMs) within the Átl’ḵa7tsem/Howe Sound Biosphere Region. Using geospatial data from BC Parcel Data, CPCAD, and ecological sensitivity datasets, the study aims to assess parcels of land for their potential to support biodiversity conservation while meeting the criteria for OECM designation. The methodology involves creating a “mesh sensitivity layer” that integrates critical habitat data, forest age (&gt;200 years), and proximity to streams, combined with parcel attributes like ownership and zoning.\nSpatial analysis, guided by a decision support tool, evaluates parcels based on ecological sensitivity, proximity to existing conservation areas, and governance capacity. The project also examines differences across regional districts (Metro Vancouver, Sunshine Coast, Squamish Lillooet), aiming to prioritize parcels that are unprotected but ecologically significant. Expected results include a map of high-priority OECM candidates, helping to enhance biodiversity conservation while contributing to Canada’s broader conservation goals under the CBD’s Aichi Targets.\n\n\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\nmain_map\n\n\n\n\n\n\n\n\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "content.html#leaflet",
    "href": "content.html#leaflet",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Sample leaflet - for detailed leaflet instructions, visit the FCOR 599 workshop archive page here.\n\nmain_map"
  },
  {
    "objectID": "content.html#code-snippets",
    "href": "content.html#code-snippets",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Sample code snippet. Notice that you can provide a toggle to switch between coding languages - this is referred to as a ‘tabset’ in quarto. It is good practice to try and convert your R code to python, and vice-versa to demonstrate coding proficiency. For example, let’s showcase a function for calculating NDVI in R and Python.\n\nRPython\n\n\ncalc_ndvi &lt;- function(nir, red){ ndvi &lt;- (nir-red)/(nir+red) return(ndvi) }\n\n\ndef calc_ndvi(nir, red): \n  ndvi = (nir.astype(float)-red.astype(float))/(nir.astype(float)+red.astype(float))\n  return(ndvi)"
  },
  {
    "objectID": "content.html#external-links",
    "href": "content.html#external-links",
    "title": "Content & Deliverables",
    "section": "",
    "text": "We can also provide a frame linking to external websites. For example, here is a link to a Google Earth Engine application I developed. The full-screen GEE application is available here in case you’re interested.\n(To use the GEE tool, navigate to any city you’d like, hit apply filters, and click anywhere on the map to retrieve a time-series of Landsat surface temperature observations for that point. Areas where the maximum temp exceeded 35 degrees Celsius in your date-range are highlighted in red.)"
  },
  {
    "objectID": "content.html#oecm-land-suitability-mesh-layer",
    "href": "content.html#oecm-land-suitability-mesh-layer",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Each hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\nmain_map"
  },
  {
    "objectID": "content.html#bc-fabric-parcel-selection",
    "href": "content.html#bc-fabric-parcel-selection",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Using the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "backup_content.html",
    "href": "backup_content.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "backup_content.html#quarto",
    "href": "backup_content.html#quarto",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "backup_content.html#running-code",
    "href": "backup_content.html#running-code",
    "title": "Content & Deliverables",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "backup_content.html#oecm-land-suitability-mesh-layer",
    "href": "backup_content.html#oecm-land-suitability-mesh-layer",
    "title": "Content & Deliverables",
    "section": "OECM Land Suitability Mesh Layer",
    "text": "OECM Land Suitability Mesh Layer\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\nmain_map"
  },
  {
    "objectID": "backup_content.html#code-snippets",
    "href": "backup_content.html#code-snippets",
    "title": "Content & Deliverables",
    "section": "Code Snippets",
    "text": "Code Snippets\nSample code snippet. Notice that you can provide a toggle to switch between coding languages - this is referred to as a ‘tabset’ in quarto. It is good practice to try and convert your R code to python, and vice-versa to demonstrate coding proficiency. For example, let’s showcase a function for calculating NDVI in R and Python.\n\nRPython\n\n\ncalc_ndvi &lt;- function(nir, red){ ndvi &lt;- (nir-red)/(nir+red) return(ndvi) }\n\n\ndef calc_ndvi(nir, red): \n  ndvi = (nir.astype(float)-red.astype(float))/(nir.astype(float)+red.astype(float))\n  return(ndvi)"
  },
  {
    "objectID": "backup_content.html#external-links",
    "href": "backup_content.html#external-links",
    "title": "Content & Deliverables",
    "section": "External links",
    "text": "External links\nWe can also provide a frame linking to external websites. For example, here is a link to a Google Earth Engine application I developed. The full-screen GEE application is available here in case you’re interested.\n(To use the GEE tool, navigate to any city you’d like, hit apply filters, and click anywhere on the map to retrieve a time-series of Landsat surface temperature observations for that point. Areas where the maximum temp exceeded 35 degrees Celsius in your date-range are highlighted in red.)"
  },
  {
    "objectID": "portfolio_personal.html",
    "href": "portfolio_personal.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_personal.html#quarto",
    "href": "portfolio_personal.html#quarto",
    "title": "Personal Projects",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_personal.html#running-code",
    "href": "portfolio_personal.html#running-code",
    "title": "Personal Projects",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Education\n\nB.Sc Earth and Environmental Science UBC Okanagan\nMasters in Geomatics for Environmental Management UBC Vancouver\n\n\n\nProfessional Experience\n\nLead Field Technician at AquaCoustic Remote Technologies Inc\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software\n\nVisitors Service Attendant for Banff National Park\n\nPublic outreach - Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively\n\n\n\n\nPublications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "portfolio_geospatial.html",
    "href": "portfolio_geospatial.html",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "This project focuses on identifying and designating Other Effective Area-Based Conservation Measures (OECMs) within the Átl’ḵa7tsem/Howe Sound Biosphere Region. Using geospatial data from BC Parcel Data, CPCAD, and ecological sensitivity datasets, the study aims to assess parcels of land for their potential to support biodiversity conservation while meeting the criteria for OECM designation. The methodology involves creating a “mesh sensitivity layer” that integrates critical habitat data, forest age (/&gt;200 years), and proximity to streams, combined with parcel attributes like ownership and zoning.\nSpatial analysis, guided by a decision support tool, evaluates parcels based on ecological sensitivity, proximity to existing conservation areas, and governance capacity. The project also examines differences across regional districts (Metro Vancouver, Sunshine Coast, Squamish Lillooet), aiming to prioritize parcels that are unprotected but ecologically significant. Expected results include a map of high-priority OECM candidates, helping to enhance biodiversity conservation while contributing to Canada’s broader conservation goals under the CBD’s Aichi Targets.\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\n\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "portfolio_geospatial.html#oecm-land-suitability-mesh-layer",
    "href": "portfolio_geospatial.html#oecm-land-suitability-mesh-layer",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Each hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM."
  },
  {
    "objectID": "portfolio_geospatial.html#bc-fabric-parcel-selection",
    "href": "portfolio_geospatial.html#bc-fabric-parcel-selection",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Using the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "portfolio_remote_sensing.html",
    "href": "portfolio_remote_sensing.html",
    "title": "Remote Sensing",
    "section": "",
    "text": "During my free time and from my Masters of Geomatics for Environmental Management (MGEM) at UBC, I have learned a range of remote sensing techniques and skills.\nBelow are some of those projects, current and past."
  },
  {
    "objectID": "portfolio_remote_sensing.html#quarto",
    "href": "portfolio_remote_sensing.html#quarto",
    "title": "Remote Sensing",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_remote_sensing.html#running-code",
    "href": "portfolio_remote_sensing.html#running-code",
    "title": "Remote Sensing",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "portfolio_landscape_ecology.html",
    "href": "portfolio_landscape_ecology.html",
    "title": "Landscape Ecology",
    "section": "",
    "text": "This page is currently a work in progress.\n\n\n  layer       crs units   class n_classes OK\n1     1 projected     m integer        17  ✔\n\n\n\n\n# A tibble: 10 × 4\n      ID Class                       Color   Description                        \n   &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;                              \n 1    11 Open Water                  #5475A8 Areas of open water, generally wit…\n 2    21 Developed, Open Space       #E8D1D1 Areas with a mixture of some const…\n 3    22 Developed, Low Intensity    #E29E8C Areas with a mixture of constructe…\n 4    23 Developed, Medium Intensity #ff0000 Areas with a mixture of constructe…\n 5    41 Deciduous Forest            #85C77E Areas dominated by trees generally…\n 6    42 Evergreen Forest            #38814E Areas dominated by trees generally…\n 7    43 Mixed Forest                #D4E7B0 Areas dominated by trees generally…\n 8    52 Shrub/Scrub                 #DCCA8F Areas dominated by shrubs; less th…\n 9    82 Cultivated Crops            #CA9146 Areas used for the production of a…\n10    90 Woody Wetlands              #C8E6F8 Areas where forest or shrubland ve…"
  },
  {
    "objectID": "portfolio_landscape_ecology.html#quarto",
    "href": "portfolio_landscape_ecology.html#quarto",
    "title": "Landscape Ecology",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_landscape_ecology.html#running-code",
    "href": "portfolio_landscape_ecology.html#running-code",
    "title": "Landscape Ecology",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "I am always eager to connect! If you have any questions, opportunities or just want to say hi, feel free to reach out through any of the channels below:\n\n\n\nemail: maxlorsignol@gmail.com\n\n\nphone: 780-340-6128\n\n\nLinkedIn: Max Lorsignol\n\n\nGitHub: maxlorsignol"
  },
  {
    "objectID": "contact.html#quarto",
    "href": "contact.html#quarto",
    "title": "Contact",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "contact.html#running-code",
    "href": "contact.html#running-code",
    "title": "Contact",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "portfolio_remote_sensing.html#examples-of-remote-sensing-experience",
    "href": "portfolio_remote_sensing.html#examples-of-remote-sensing-experience",
    "title": "Remote Sensing",
    "section": "",
    "text": "Below are some projects in remote sensing techniques that I have become familiar with during my Masters program at UBC."
  },
  {
    "objectID": "portfolio_remote_sensing.html#land-classification",
    "href": "portfolio_remote_sensing.html#land-classification",
    "title": "Remote Sensing",
    "section": "Land Classification",
    "text": "Land Classification\nThe Remote Sensing course in MGEM taught me how to produce landscape classifications using a variety of supervised and unsupervised classification processes. Fascinated by different classification processes and the maps it produced, I extended this skill to produce a number of classified maps of various landscapes where Landsat and Copernicus satellite data was available. Generally my focus was with the Howe Sound Biosphere as it was part of my capstone project at UBC and is where I spend my time hiking, skiing and climbing. Below are some of the results of recent pixel classifications.\nBelow is a slide-by-side comparison of two different classification processes of the Howe Sound. The left side is classified under a supervised process using hand-delineated polygons, 20/80 training to validation, with custom R code (a lot of work). The right shows a classification using the ArcGIS Pro deeplearning land cover classification (almost instantaneous).\n\n\n\n\n\n\nThe idea here is to visualize the difference and address the accuracies between the two classification models. How do you think they compare?"
  },
  {
    "objectID": "portfolio_remote_sensing.html#time-series-analysis",
    "href": "portfolio_remote_sensing.html#time-series-analysis",
    "title": "Remote Sensing",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nIn this lab, I analyzed time series data in R by first working with MODIS NDVI data. I applied the BFAST algorithm to detect abrupt changes in vegetation—like logging events—that cause rapid declines in NDVI values. I then examined a 33‐year series of land cover maps from Landsat imagery, reclassifying the data into forested and non-forested areas to evaluate trends in forest loss and gain. Throughout the lab, I developed R code for data extraction, time series creation, and visualization, showcasing my practical skills in spatial and temporal analysis with packages such as terra, sf, and bfast."
  },
  {
    "objectID": "portfolio_remote_sensing.html#burn-severity-analysis",
    "href": "portfolio_remote_sensing.html#burn-severity-analysis",
    "title": "Remote Sensing",
    "section": "Burn Severity Analysis",
    "text": "Burn Severity Analysis"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "B.Sc Earth and Environmental Science UBC Okanagan (2024-2025)\nMasters in Geomatics for Environmental Management UBC Vancouver (2016-2019)"
  },
  {
    "objectID": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2021-2024",
    "href": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2021-2024",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc 2021-2024",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc 2021-2024\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv.html#visitors-service-attendant-for-banff-national-park",
    "href": "cv.html#visitors-service-attendant-for-banff-national-park",
    "title": "CV",
    "section": "Visitors Service Attendant for Banff National Park",
    "text": "Visitors Service Attendant for Banff National Park\n(Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively"
  },
  {
    "objectID": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2020-2024",
    "href": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2020-2024",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc (2020-2024)",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc (2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv.html#co-founder-canopy-campers-van-rentals-2023---present",
    "href": "cv.html#co-founder-canopy-campers-van-rentals-2023---present",
    "title": "CV",
    "section": "Co-Founder Canopy Campers Van Rentals (2023 - Present)",
    "text": "Co-Founder Canopy Campers Van Rentals (2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking"
  },
  {
    "objectID": "cv.html#visitors-service-attendant-for-banff-national-park-summers-2018-and-2019",
    "href": "cv.html#visitors-service-attendant-for-banff-national-park-summers-2018-and-2019",
    "title": "CV",
    "section": "Visitors Service Attendant for Banff National Park (Summers 2018 and 2019)",
    "text": "Visitors Service Attendant for Banff National Park (Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively"
  },
  {
    "objectID": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "href": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv.html#co-founder-canopy-campers-van-rentals",
    "href": "cv.html#co-founder-canopy-campers-van-rentals",
    "title": "CV",
    "section": "Co-Founder Canopy Campers Van Rentals",
    "text": "Co-Founder Canopy Campers Van Rentals\n(2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking"
  },
  {
    "objectID": "portfolio_landscape_ecology.html#marbled-murrelet-landscape-connectivity-using-grainsofconnectivity",
    "href": "portfolio_landscape_ecology.html#marbled-murrelet-landscape-connectivity-using-grainsofconnectivity",
    "title": "Landscape Ecology",
    "section": "",
    "text": "This page is currently a work in progress.\n\n\n  layer       crs units   class n_classes OK\n1     1 projected     m integer        17  ✔\n\n\n\n\n# A tibble: 10 × 4\n      ID Class                       Color   Description                        \n   &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;                              \n 1    11 Open Water                  #5475A8 Areas of open water, generally wit…\n 2    21 Developed, Open Space       #E8D1D1 Areas with a mixture of some const…\n 3    22 Developed, Low Intensity    #E29E8C Areas with a mixture of constructe…\n 4    23 Developed, Medium Intensity #ff0000 Areas with a mixture of constructe…\n 5    41 Deciduous Forest            #85C77E Areas dominated by trees generally…\n 6    42 Evergreen Forest            #38814E Areas dominated by trees generally…\n 7    43 Mixed Forest                #D4E7B0 Areas dominated by trees generally…\n 8    52 Shrub/Scrub                 #DCCA8F Areas dominated by shrubs; less th…\n 9    82 Cultivated Crops            #CA9146 Areas used for the production of a…\n10    90 Woody Wetlands              #C8E6F8 Areas where forest or shrubland ve…"
  },
  {
    "objectID": "portfolio_geospatial.html#work-in-progress",
    "href": "portfolio_geospatial.html#work-in-progress",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "I am currently working on a webmap that allows for the users to dynamically adjust the importance of different layers and immediately see how those changes affect the selection of OECM parcels.\nHere is a Python code snippet.\n\nPython\n\n\n``` (.python, echo=false) import rasterio import numpy as np import geopandas as gpd import matplotlib.pyplot as plt from rasterio.plot import show from rasterio.warp import reproject, Resampling\n\n\nraster_files = { “biological_sensitivity”: r”c:.stuWebApp_habitat_raster.tif”, “watershed”: r”c:.stuWebApp_raster.tif”, “streams”: r”c:.stuWebApp_50m_buffer.tif”, “wetlands”: r”c:.stuWebApp_raster.tif”, “slope”: r”c:.stuWebApp_greaterthan30_raster.tif”, “forest_age”: r”c:.stuWebApp_age_vri.tif” }\n\n\n\nweights = { “biological_sensitivity”: 0.25, “watershed”: 0.15, “streams”: 0.2, “wetlands”: 0.15, “slope”: 0.1, “forest_age”: 0.15 }\n\n\n\nreference_raster = list(raster_files.values())[0]\n\n\n\nwith rasterio.open(reference_raster) as ref_src: ref_transform = ref_src.transform ref_crs = ref_src.crs ref_width = ref_src.width ref_height = ref_src.height ref_dtype = ref_src.dtypes[0]\n\n\n\ndef resample_raster(src_path): with rasterio.open(src_path) as src: data = src.read(1).astype(float) # Read band 1 data[data == src.nodata] = np.nan # Handle NoData values\n    # Create an empty array for resampled data\n    resampled_data = np.empty((ref_height, ref_width), dtype=ref_dtype)\n    \n    # Reproject and resample\n    reproject(\n        source=data,\n        destination=resampled_data,\n        src_transform=src.transform,\n        src_crs=src.crs,\n        dst_transform=ref_transform,\n        dst_crs=ref_crs,\n        resampling=Resampling.bilinear  # Change to nearest for categorical data\n    )\n    \n    return resampled_data\n\n\n\nweighted_sum = np.zeros((ref_height, ref_width), dtype=np.float32) # Initialize empty array\nfor layer, path in raster_files.items(): resampled_raster = resample_raster(path)\nif resampled_raster.shape != (ref_height, ref_width):\n    raise ValueError(f\"Resampled raster '{layer}' has incorrect shape {resampled_raster.shape}\")\n\nweighted_sum += weights[layer] * resampled_raster  # Apply weighted sum\n\n\n\nweighted_sum = (weighted_sum - np.nanmin(weighted_sum)) / (np.nanmax(weighted_sum) - np.nanmin(weighted_sum))\n\n\n\nparcels = gpd.read_file(r”c:.stuWebApp_Parcel_all.shp”)\n\n\n\nthreshold = np.nanpercentile(weighted_sum, 90) # Adjust as needed selected_parcels = parcels.copy() selected_parcels[“suitability”] = selected_parcels.geometry.centroid.apply( lambda pt: weighted_sum[int(pt.y), int(pt.x)] if 0 &lt;= int(pt.y) &lt; ref_height and 0 &lt;= int(pt.x) &lt; ref_width and weighted_sum[int(pt.y), int(pt.x)] &gt;= threshold else np.nan ) selected_parcels = selected_parcels.dropna()\n\n\n\n\n\n\nwith rasterio.open(reference_raster) as src: raster_extent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n\n\n\nparcel_bounds = parcels.total_bounds # [minx, miny, maxx, maxy]\n\n\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n\n\nshow(weighted_sum, transform=ref_transform, cmap=“viridis”, ax=ax, title=“Weighted Suitability Map”)\n\n\n\nparcels.boundary.plot(ax=ax, edgecolor=“gray”, linewidth=0.5)\n\n\n\nselected_parcels.boundary.plot(ax=ax, edgecolor=“red”, linewidth=1.5, label=“Selected Parcels”)\n\n\n\nax.set_xlim(parcel_bounds[0], parcel_bounds[2]) ax.set_ylim(parcel_bounds[1], parcel_bounds[3])\nplt.legend() plt.show() ```\n\n\n\n\n\n\n\nHoweSoundParcels"
  },
  {
    "objectID": "portfolio_geospatial.html#python",
    "href": "portfolio_geospatial.html#python",
    "title": "Geospatial Analysis",
    "section": "Python",
    "text": "Python\n\n\nPython Code Snippet\nimport rasterio\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfrom rasterio.warp import reproject, Resampling\n\n# Define input raster files\nraster_files = {\n    \"biological_sensitivity\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/critical_habitat_raster.tif\",\n    \"watershed\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/watershed_raster.tif\",\n    \"streams\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/streams_50m_buffer.tif\",\n    \"wetlands\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/wetland_raster.tif\",\n    \"slope\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/slope_greaterthan30_raster.tif\",\n    \"forest_age\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/forest_age_vri.tif\"\n}\n\n# Define weights\nweights = {\n    \"biological_sensitivity\": 0.25,\n    \"watershed\": 0.15,\n    \"streams\": 0.2,\n    \"wetlands\": 0.15,\n    \"slope\": 0.1,\n    \"forest_age\": 0.15\n}\n\n# Select a reference raster (first raster in the dictionary)\nreference_raster = list(raster_files.values())[0]\n\n# Read reference raster to get target shape, transform, and CRS\nwith rasterio.open(reference_raster) as ref_src:\n    ref_transform = ref_src.transform\n    ref_crs = ref_src.crs\n    ref_width = ref_src.width\n    ref_height = ref_src.height\n    ref_dtype = ref_src.dtypes[0]\n\n# Function to resample rasters to match the reference raster dimensions\ndef resample_raster(src_path):\n    with rasterio.open(src_path) as src:\n        data = src.read(1).astype(float)\n        data[data == src.nodata] = np.nan  # Handle NoData values\n        \n        # Create an empty array for resampled data\n        resampled_data = np.empty((ref_height, ref_width), dtype=ref_dtype)\n        \n        # Reproject and resample\n        reproject(\n            source=data,\n            destination=resampled_data,\n            src_transform=src.transform,\n            src_crs=src.crs,\n            dst_transform=ref_transform,\n            dst_crs=ref_crs,\n            resampling=Resampling.bilinear\n        )\n        \n        return resampled_data\n\n# Read, resample, and apply weights\nweighted_sum = np.zeros((ref_height, ref_width), dtype=np.float32)\n\nfor layer, path in raster_files.items():\n    resampled_raster = resample_raster(path)\n    weighted_sum += weights[layer] * resampled_raster  # Apply weighted sum\n\n# Normalize weighted sum\nweighted_sum = (weighted_sum - np.nanmin(weighted_sum)) / (np.nanmax(weighted_sum) - np.nanmin(weighted_sum))\n\n# Load parcel data\nparcels = gpd.read_file(r\"c:/Users/lorsigno.stu/Documents/Projects/OECM_WebApp/Layers/HoweSound_Parcel_all.shp\")\n\n# Select top X% parcels based on suitability\nthreshold = np.nanpercentile(weighted_sum, 90)\nselected_parcels = parcels.copy()\nselected_parcels[\"suitability\"] = selected_parcels.geometry.centroid.apply(\n    lambda pt: weighted_sum[int(pt.y), int(pt.x)]\n    if 0 &lt;= int(pt.y) &lt; ref_height and 0 &lt;= int(pt.x) &lt; ref_width and weighted_sum[int(pt.y), int(pt.x)] &gt;= threshold\n    else np.nan\n)\nselected_parcels = selected_parcels.dropna()\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot weighted suitability raster\nshow(weighted_sum, transform=ref_transform, cmap=\"viridis\", ax=ax, title=\"Howe Sound Parcel Map\")\n\n# Plot parcels\nparcels.boundary.plot(ax=ax, edgecolor=\"gray\", linewidth=0.5)\n\n# Plot selected parcels\nselected_parcels.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=1.5, label=\"Selected Parcels\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nHowe Sound Biosphere Parcels"
  },
  {
    "objectID": "cv2.html",
    "href": "cv2.html",
    "title": "CV",
    "section": "",
    "text": "Click Here to download the PDF"
  },
  {
    "objectID": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "href": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv2.html#co-founder-canopy-campers-van-rentals",
    "href": "cv2.html#co-founder-canopy-campers-van-rentals",
    "title": "CV",
    "section": "Co-Founder Canopy Campers Van Rentals",
    "text": "Co-Founder Canopy Campers Van Rentals\n(2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking"
  },
  {
    "objectID": "cv2.html#visitors-service-attendant-for-banff-national-park",
    "href": "cv2.html#visitors-service-attendant-for-banff-national-park",
    "title": "CV",
    "section": "Visitors Service Attendant for Banff National Park",
    "text": "Visitors Service Attendant for Banff National Park\n(Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively"
  },
  {
    "objectID": "BurnSeverity.html",
    "href": "BurnSeverity.html",
    "title": "Burn Severity",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n#Attach packages here\nlibrary(terra)\n\nWarning: package 'terra' was built under R version 4.3.3\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.3.3\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n#| execute: true\n#| eval: true\n#| code-fold: true\n#| code-summary: \"R Code Snippet\"\n\n\n# read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\")\n\nReading layer `H_FIRE_PLY_polygon' from data source \n  `C:\\Users\\lorsigno.stu\\OneDrive - UBC\\Desktop\\Documents\\GEM 520\\Lab 8\\lab8_finalDemonstrationOfRSkills-assign\\data\\PROT_HISTORICAL_FIRE_POLYS_SP\\H_FIRE_PLY_polygon.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22479 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 408933.1 ymin: 370244.2 xmax: 1870587 ymax: 1709027\nProjected CRS: NAD83 / BC Albers\n\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'FIRE_YEAR'. You can override using the\n`.groups` argument.\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nR Code Snippet\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\n# i need to connect the directory, then assign the bands, then assign the bands a color, crop, then plot\npre_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\"\n\npre_fire_RGB &lt;- rast(pre_fire_dir)\n\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\n\n# post-fire is July 15 2018\npost_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\"\npost_fire_RGB &lt;- rast(post_fire_dir)\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\n\n# plot\n\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\nR Code Snippet\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nR Code Snippet\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nR Code Snippet\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\nWarning in plot.sf(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, :\nignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nR Code Snippet\nwriteRaster(pre_fire_nbr, \"data/pre_fire_nbr.tif\", overwrite = TRUE)\nwriteRaster(post_fire_nbr, \"data/post_fire_nbr.tif\", overwrite = TRUE)\n\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nR Code Snippet\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code Snippet\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "OECMParcelSelection.html",
    "href": "OECMParcelSelection.html",
    "title": "OECM Parcel Selection",
    "section": "",
    "text": "This project focuses on identifying and designating Other Effective Area-Based Conservation Measures (OECMs) within the Átl’ḵa7tsem/Howe Sound Biosphere Region. Using geospatial data from a variety of sources, the study aims to assess parcels of land for their potential to support biodiversity conservation while meeting the criteria for OECM designation. The methodology involves creating a weighted suitability model that integrates critical habitat data, forest age, slope, streams buffer, parks buffer, wetlands and watersheds to make an initial parcel selection from the BC Parcel Fabric. Inital parcels are then further filtered by parcel attributes like ownership, class and zoning.\nIn the Weighted Layer, each hexagon has a value based on the a weighted suitability model. Darker values represent more sensitive areas and lighter values are less sensitive.\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh based on a specified threshold. These parcels are then categorized based on their zoning, OCP and municipality. The resulting parcels can be seen below.\n\n\nLoaded: bc_parcel \n\n\nLoaded: protected_areas \n\n\nLoaded: all_oecm_parcels \n\n\nRaster loaded successfully\n\n\n\n\n\n\nPlease be patient with the interactive map, as the weighted suitability layer is quite large. Feel free to toggle off some layers if this page is running poorly."
  },
  {
    "objectID": "OECMParcelSelection.html#bc-fabric-parcel-selection",
    "href": "OECMParcelSelection.html#bc-fabric-parcel-selection",
    "title": "OECM Parcel Selection",
    "section": "BC Fabric Parcel Selection",
    "text": "BC Fabric Parcel Selection\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change.\n\n\nLoaded: bc_parcel \n\n\nLoaded: scrd_parcels \nLoaded: mvrd_parcels \nLoaded: slrd_parcels \n\n\nRaster loaded successfully\n\n\n\n\n\n\nPlease be patient with the interactive map, as the weighted suitability layer is quite large. Feel free to toggle off some layers if this page is running poorly."
  },
  {
    "objectID": "OECMParcelSelection.html#python",
    "href": "OECMParcelSelection.html#python",
    "title": "OECM Parcel Selection",
    "section": "Python",
    "text": "Python\n\n\nPython Code Snippet\nimport rasterio\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfrom rasterio.warp import reproject, Resampling\n\n# Define input raster files\nraster_files = {\n    \"biological_sensitivity\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/critical_habitat_raster.tif\",\n    \"watershed\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/watershed_raster.tif\",\n    \"streams\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/streams_50m_buffer.tif\",\n    \"wetlands\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/wetland_raster.tif\",\n    \"slope\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/slope_greaterthan30_raster.tif\",\n    \"forest_age\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/forest_age_vri.tif\"\n}\n\n# Define weights\nweights = {\n    \"biological_sensitivity\": 0.25,\n    \"watershed\": 0.15,\n    \"streams\": 0.2,\n    \"wetlands\": 0.15,\n    \"slope\": 0.1,\n    \"forest_age\": 0.15\n}\n\n# Select a reference raster (first raster in the dictionary)\nreference_raster = list(raster_files.values())[0]\n\n# Read reference raster to get target shape, transform, and CRS\nwith rasterio.open(reference_raster) as ref_src:\n    ref_transform = ref_src.transform\n    ref_crs = ref_src.crs\n    ref_width = ref_src.width\n    ref_height = ref_src.height\n    ref_dtype = ref_src.dtypes[0]\n\n# Function to resample rasters to match the reference raster dimensions\ndef resample_raster(src_path):\n    with rasterio.open(src_path) as src:\n        data = src.read(1).astype(float)\n        data[data == src.nodata] = np.nan  # Handle NoData values\n        \n        # Create an empty array for resampled data\n        resampled_data = np.empty((ref_height, ref_width), dtype=ref_dtype)\n        \n        # Reproject and resample\n        reproject(\n            source=data,\n            destination=resampled_data,\n            src_transform=src.transform,\n            src_crs=src.crs,\n            dst_transform=ref_transform,\n            dst_crs=ref_crs,\n            resampling=Resampling.bilinear\n        )\n        \n        return resampled_data\n\n# Read, resample, and apply weights\nweighted_sum = np.zeros((ref_height, ref_width), dtype=np.float32)\n\nfor layer, path in raster_files.items():\n    resampled_raster = resample_raster(path)\n    weighted_sum += weights[layer] * resampled_raster  # Apply weighted sum\n\n# Normalize weighted sum\nweighted_sum = (weighted_sum - np.nanmin(weighted_sum)) / (np.nanmax(weighted_sum) - np.nanmin(weighted_sum))\n\n# Load parcel data\nparcels = gpd.read_file(r\"c:/Users/lorsigno.stu/Documents/Projects/OECM_WebApp/Layers/HoweSound_Parcel_all.shp\")\n\n# Select top X% parcels based on suitability\nthreshold = np.nanpercentile(weighted_sum, 90)\nselected_parcels = parcels.copy()\nselected_parcels[\"suitability\"] = selected_parcels.geometry.centroid.apply(\n    lambda pt: weighted_sum[int(pt.y), int(pt.x)]\n    if 0 &lt;= int(pt.y) &lt; ref_height and 0 &lt;= int(pt.x) &lt; ref_width and weighted_sum[int(pt.y), int(pt.x)] &gt;= threshold\n    else np.nan\n)\nselected_parcels = selected_parcels.dropna()\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot weighted suitability raster\nshow(weighted_sum, transform=ref_transform, cmap=\"viridis\", ax=ax, title=\"Howe Sound Parcel Map\")\n\n# Plot parcels\nparcels.boundary.plot(ax=ax, edgecolor=\"gray\", linewidth=0.5)\n\n# Plot selected parcels\nselected_parcels.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=1.5, label=\"Selected Parcels\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nHowe Sound Biosphere Parcels"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "During my free time and from my Masters of Geomatics for Environmental Management (MGEM) at UBC, I have learned a range of remote sensing techniques and skills.\nBelow are some of those projects, current and past.\n\nLand Classification\nThe Remote Sensing course in MGEM taught me how to produce landscape classifications using a variety of supervised and unsupervised classification processes. Fascinated by different classification processes and the maps it produced, I extended this skill to produce a number of classified maps of various landscapes where Landsat and Copernicus satellite data was available. Generally my focus was with the Howe Sound Biosphere as it was part of my capstone project at UBC and is where I spend my time hiking, skiing and climbing. Below are some of the results of recent pixel classifications.\nBelow is a slide-by-side comparison of two different classification processes of the Howe Sound. The left side is classified under a supervised process using hand-delineated polygons, 20/80 training to validation, with custom R code. The right shows a classification using the ArcGIS Pro deeplearning land cover classification. The idea here is to visualize the difference and address the accuracies between the two classification models. How do you think they compare?"
  },
  {
    "objectID": "classification.html#land-classification",
    "href": "classification.html#land-classification",
    "title": "Classification",
    "section": "Land Classification",
    "text": "Land Classification\nThe Remote Sensing course in MGEM taught me how to produce landscape classifications using a variety of supervised and unsupervised classification processes. Fascinated by different classification processes and the maps it produced, I extended this skill to produce a number of classified maps of various landscapes where Landsat and Copernicus satellite data was available. Generally my focus was with the Howe Sound Biosphere as it was part of my capstone project at UBC and is where I spend my time hiking, skiing and climbing. Below are some of the results of recent pixel classifications.\nBelow is a slide-by-side comparison of two different classification processes of the Howe Sound. The left side is classified under a supervised process using hand-delineated polygons, 20/80 training to validation, with custom R code (a lot of work). The right shows a classification using the ArcGIS Pro deeplearning land cover classification (almost instantaneous).\n\n\n\n\n\n\nThe idea here is to visualize the difference and address the accuracies between the two classification models. How do you think they compare?"
  },
  {
    "objectID": "BurnSeverity_Modified.html",
    "href": "BurnSeverity_Modified.html",
    "title": "Burn Severity",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n\nCode\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#Attach packages\nsuppressPackageStartupMessages({\n  library(terra)\n  library(sf)\n  library(dplyr)\n  library(ggplot2)\n  library(readr)\n  library(stringr)\n  library(lubridate)\n  library(tidyr)\n})\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n\nCode\n#| execute: true\n#| eval: true\n#| echo: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n\n#read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\")\n\n\nReading layer `H_FIRE_PLY_polygon' from data source \n  `C:\\Users\\lorsigno.stu\\OneDrive - UBC\\Desktop\\Documents\\GEM 520\\Lab 8\\lab8_finalDemonstrationOfRSkills-assign\\data\\PROT_HISTORICAL_FIRE_POLYS_SP\\H_FIRE_PLY_polygon.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22479 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 408933.1 ymin: 370244.2 xmax: 1870587 ymax: 1709027\nProjected CRS: NAD83 / BC Albers\n\n\nCode\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\n# i need to connect the directory, then assign the bands, then assign the bands a color, crop, then plot\npre_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\"\n\npre_fire_RGB &lt;- rast(pre_fire_dir)\n\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\n\n# post-fire is July 15 2018\npost_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\"\npost_fire_RGB &lt;- rast(post_fire_dir)\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\n\n# plot\n\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\nCode\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\nWarning in plot.sf(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, :\nignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nCode\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "burnseverity.html",
    "href": "burnseverity.html",
    "title": "Burn Severity Analysis of Prouton Lakes Fire",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n\nCode\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#Attach packages\nsuppressPackageStartupMessages({\n  library(terra)\n  library(sf)\n  library(dplyr)\n  library(ggplot2)\n  library(readr)\n  library(stringr)\n  library(lubridate)\n  library(tidyr)\n})\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n\nCode\n#| execute: true\n#| eval: true\n#| echo: false\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\", quiet = TRUE)\n\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  #sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  #ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  #nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  #writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  #writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  #writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\npre_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\")\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Pre-Fire (2015)\")\n\n\n\n\n\n\n\n\n\nCode\n# post-fire is July 15 2018\npost_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\")\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Post-Fire (2018)\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\nWarning in plot.sf(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, :\nignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nCode\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "lidrforestmodel.html",
    "href": "lidrforestmodel.html",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "",
    "text": "Show/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lidrforestmodel.html#load-in-necessary-packages",
    "href": "lidrforestmodel.html#load-in-necessary-packages",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "",
    "text": "Show/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lidrforestmodel.html#read-in-data",
    "href": "lidrforestmodel.html#read-in-data",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "Read in Data",
    "text": "Read in Data\nSet working directory\nRead in the relevant .csv file\n\n\nShow/Hide Code\nwork_dir &lt;- \"~/GEM 521/Lab 2/Data\" # set file path to Lab 2 data\nsetwd(work_dir) # set working directory to that file path\n\nplot_table &lt;- read_csv(\"~/GEM 521/Lab 2/Data/Plots/Plot_Table.csv\") # .csv of the plot locations and \nmkrf_plot_metrics &lt;- read_csv(\"~/GEM 521/Lab 2/Data/Plots/mkrf_plot_metrics.csv\") # .csv metrics of the lidar point cloud\n\n#Add column to \"mkrf_plot_metrics' called Plot_ID (join key)\nmkrf_plot_metrics$Plot_ID = 1:20 # create a column with values 1-20 to join the plot_table to based on the ID.\n\n#Join 'Plot_Table' and 'MKRF_Plot_Metrics' into 'data_table'\ndata_table &lt;- plot_table %&gt;% \n  full_join(mkrf_plot_metrics) # join the plot_table to mkrf_plot in a seperate frame called data_table.\n\n\nNow we have a data frame with all the metrics from both the LiDAR and plot."
  },
  {
    "objectID": "lidrforestmodel.html#investigate-metrics",
    "href": "lidrforestmodel.html#investigate-metrics",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "Investigate Metrics",
    "text": "Investigate Metrics\n\n\nShow/Hide Code\n# list all the column names in data_table\ncolnames(data_table)\n\n\n [1] \"Plot_ID\"      \"X\"            \"Y\"            \"Net_Volume\"   \"...1\"        \n [6] \"zmax\"         \"zmean\"        \"zsd\"          \"zskew\"        \"zkurt\"       \n[11] \"zentropy\"     \"pzabovezmean\" \"pzabove2\"     \"zq5\"          \"zq10\"        \n[16] \"zq15\"         \"zq20\"         \"zq25\"         \"zq30\"         \"zq35\"        \n[21] \"zq40\"         \"zq45\"         \"zq50\"         \"zq55\"         \"zq60\"        \n[26] \"zq65\"         \"zq70\"         \"zq75\"         \"zq80\"         \"zq85\"        \n[31] \"zq90\"         \"zq95\"         \"zpcum1\"       \"zpcum2\"       \"zpcum3\"      \n[36] \"zpcum4\"       \"zpcum5\"       \"zpcum6\"       \"zpcum7\"       \"zpcum8\"      \n[41] \"zpcum9\"       \"itot\"         \"imax\"         \"imean\"        \"isd\"         \n[46] \"iskew\"        \"ikurt\"        \"ipground\"     \"ipcumzq10\"    \"ipcumzq30\"   \n[51] \"ipcumzq50\"    \"ipcumzq70\"    \"ipcumzq90\"    \"p1th\"         \"p2th\"        \n[56] \"p3th\"         \"p4th\"         \"p5th\"         \"pground\"      \"n\"           \n[61] \"area\"        \n\n\nWow look at all those metrics! We have our XY coordinates, net volume estimated out in the field at each of these plots, and all sorts of metrics that are automatically derived from the LiDAR data. Lets use these metrics to compute some statistics and eventually a model to estimate attributes of the forest.\nLets start with ploting the relationship between LiDAR metrics and volume.\n\n\nShow/Hide Code\npar(mfrow = c(2,3))  # Set layout to 2 rows and 3 columns\n\n# plot relationship between lidar metrics and volume.\nplot(Net_Volume ~ zq50, data = data_table) # ploting y as net_volume and x as zq50 (50th percentile height)\nplot(Net_Volume ~ zmax, data = data_table) # Maximum height of the LiDAR returns, representing the tallest point in the dataset\nplot(Net_Volume ~ zmean, data = data_table) # Mean height of all LiDAR returns, providing an average canopy or vegetation height.\nplot(Net_Volume ~ zkurt, data = data_table) # Kurtosis of the height distribution, describing the sharpness or flatness of the distribution curve.\nplot(Net_Volume ~ zsd, data = data_table) # Standard deviation of return heights, indicating variability in the canopy height structure.\nplot(Net_Volume ~ zpcum5, data = data_table) # cumulative percentage of returns below the 50th percentile (median height).\n\npar(mfrow = c(1,1))  # Reset to single plotting\n\n\n\n\n\nPlotted lidar metrics related to the volume"
  },
  {
    "objectID": "lidrforestmodel.html#comparing-models",
    "href": "lidrforestmodel.html#comparing-models",
    "title": "Estimating Forest Attributes from Lidar Metrics",
    "section": "Comparing Models",
    "text": "Comparing Models\nOkay, now that we have explored some of the metrics and how they relate to the measured volume of the trees within the plots, lets fit some linear models in those plots to see the relationship of those two variables and see if there is a significance between the two.\nFor this example, we will use the metrics zq90, pzabove2, zentropy and zskew in our model./ zq90 is the 90th percentile of lidar points, generally the upper canopy./ pzabove2 is the proportion of points above 2 meters, ei above the ground./ zentropy is the height distribution entropy, reflecting the complexity of vertical structure./ zsqew represents the skewness of height distribution showing whether the distribution is asymmetric./\nFirst lets start with a model with no variable so we can document how the model changes.\n\n\nCode\n#| execute: true\n#| eval: true\n#| echo: false\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n## MODEL A\n\n# no variables in the model\nmodelA &lt;- lm(Net_Volume ~ 1, data = data_table)\n\n# added metrics to the model to predict volume\nadd1(modelA, ~ zq90 + pzabove2 + zentropy + zskew, test = 'F') # Compute all the single terms in the scope argument\n\n\nSingle term additions\n\nModel:\nNet_Volume ~ 1\n         Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                785465 213.57                      \nzq90      1    569694 215771 189.72 47.5249 1.905e-06 ***\npzabove2  1    236749 548716 208.39  7.7663  0.012176 *  \nzentropy  1     46746 738718 214.34  1.1390  0.299959    \nzskew     1    257447 528018 207.62  8.7763  0.008338 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# add zq90 into the model\nmodelA &lt;- lm(Net_Volume ~ zq90, data = data_table) # added zq90 to the model\n\nadd1(modelA, ~ zq90 + pzabove2 + zentropy + zskew, test = 'F')\n\n\nSingle term additions\n\nModel:\nNet_Volume ~ zq90\n         Df Sum of Sq    RSS    AIC F value  Pr(&gt;F)  \n&lt;none&gt;                215771 189.72                  \npzabove2  1     47061 168710 186.80  4.7420 0.04381 *\nzentropy  1     33616 182155 188.34  3.1373 0.09444 .\nzskew     1     25764 190007 189.18  2.3051 0.14732  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# add pzabove2 to model\nmodelA &lt;- lm(Net_Volume ~ zq90 + pzabove2, data = data_table) # adding pzabove2 to the current model\n\nadd1(modelA,~ zq90 + pzabove2 + zentropy + zskew, test = 'F')\n\n\nSingle term additions\n\nModel:\nNet_Volume ~ zq90 + pzabove2\n         Df Sum of Sq    RSS    AIC F value Pr(&gt;F)\n&lt;none&gt;                168710 186.80               \nzentropy  1     13759 154951 187.10  1.4207 0.2507\nzskew     1     13230 155481 187.17  1.3614 0.2604\n\n\nCode\n# final model summary statistics\nsummary(modelA)\n\n\n\nCall:\nlm(formula = Net_Volume ~ zq90 + pzabove2, data = data_table)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-153.548  -61.170   -5.696   40.334  277.285 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.618e+05  7.418e+04  -2.181   0.0435 *  \nzq90         1.744e+01  2.818e+00   6.188 9.92e-06 ***\npzabove2     1.616e+03  7.422e+02   2.178   0.0438 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 99.62 on 17 degrees of freedom\nMultiple R-squared:  0.7852,    Adjusted R-squared:  0.7599 \nF-statistic: 31.07 on 2 and 17 DF,  p-value: 2.1e-06"
  },
  {
    "objectID": "lidrforestmodel.html#model",
    "href": "lidrforestmodel.html#model",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "Model",
    "text": "Model\nNow that we have the lidar metrics, lets build a model that can estimate the Net_Volume.\n\n\nShow/Hide Code\n## MODEL A\nmodelA &lt;- lm(Net_Volume ~ zq25, data = data_table) \n\n#Get the output coefficients to our model\nmodelA$coefficients\n\n\n(Intercept)        zq25 \n -139.44524    24.95973 \n\n\nShow/Hide Code\n# final model summary statistics\nsummary(modelA)\n\n\n\nCall:\nlm(formula = Net_Volume ~ zq25, data = data_table)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-139.56  -52.90  -12.56   32.47  231.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -139.445     52.588  -2.652   0.0162 *  \nzq25          24.960      2.791   8.942 4.85e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.54 on 18 degrees of freedom\nMultiple R-squared:  0.8163,    Adjusted R-squared:  0.8061 \nF-statistic: 79.97 on 1 and 18 DF,  p-value: 4.846e-08\n\n\nNow that we have developed a model, we will apply the model over our entire study area To do this, we must calculate LiDAR metrics over the entire study area on a grid. If we use ModelA, we have to calculate the 25th percentile.\nWe will use pixel_metrics function in lidR and the app function in terra.\nLets calculate grid_metrics for all MKRF. We need to calculate the zq25 metric for each pixel because depending on the template it can be for each pixel of a raster (area-based approach), or each polygon, or each segmented tree, or on the whole point cloud. In our case we will be doing every pixel. For the time sake, the spatial resolution will be set to 10 meters.\n\n\nShow/Hide Code\n# Calculate grid_metrics for all MKRF \n# Create LAScatalog of filtered, normalized tiles with points 2 m - 65 m \nnorm_cat_mkrf &lt;- readLAScatalog(\"~/GEM 521/Lab 2/Data/Normalized\") # read in Normalized LAS file from Lab2\nopt_filter(norm_cat_mkrf) &lt;- '-keep_z_above 2 -drop_z_above 65' # remove points above 65m and below 2 meters\n\n#Calculate grid metrics of mean Z at 10 m resolution for entire study area\npixel_metrics_mkrf &lt;- pixel_metrics(norm_cat_mkrf, .stdmetrics_z, 10) \nplot(pixel_metrics_mkrf)\n\n\n\n\n\nAll of the MKRF pixel metrics\n\n\nLook at all the rasters we have created. We can plot individual rasters using variations of plot(pixel_metrics_mkrf). For this model, we want to extract the zq25 raster from this SpatRaster. We can do that by using the subset function in the terra package. Let’s plot the extracted SpatRaster to confirm it was successful.\nAfter extracting the 25th percentile for each pixel across the study area we can apply Model 2. To do this, we first need to write a function that will apply this equation to all pixels in zq25_mkrf. The function we will create is based on the coefficients derived from Model 2. And lets apply that function to the entire MKRF study area.\n\n\nShow/Hide Code\nzq25_mkrf &lt;- terra::subset(pixel_metrics_mkrf, \"zq25\") # assign the subset zq25 metric to a variable\n\n# function based on Model2 (ModelB) coefficients\nf &lt;- function(x) {\n  24.96* x -139.45\n}\n\n#Apply function to raster\nnet_volume_mkrf &lt;- terra::app(zq25_mkrf, f) # apply function f to the zq25 metric across the entire area\nplot(net_volume_mkrf,\n     main = 'Estimated Volume using ModelB',) # plot, in 2D, the estimated volume based on our model\n\n\n\n\n\n2D visualization of the Volume based on the model"
  },
  {
    "objectID": "lidr3dmodeling.html",
    "href": "lidr3dmodeling.html",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "",
    "text": "In this report, I focus on LiDAR data processing, forest attribute estimation, and model development using R. I begin by loading the necessary libraries, including lidR, terra, tidyverse, MASS, and corrplot, to facilitate LiDAR data processing, geospatial raster operations, statistical modeling, and visualization.\nTo prepare the data, I read LiDAR tiles (.las files) into a LAScatalog object, filter and normalize the point cloud using DEM-based normalization, and extract individual plots while ensuring the removal of duplicate points. I then generate Digital Elevation Models (DEM) and Canopy Height Models (CHM) to provide spatial context for further analysis.\nFollowing this, I compute standard LiDAR cloud metrics such as mean height (zmean), maximum height (zmax), median height (zq50), and standard deviation (zsd). These metrics allow me to analyze forest structure across individual plots and the entire study area. I explore relationships between these LiDAR metrics and vegetation attributes, visualizing correlations and patterns.\nTo estimate forest attributes, I develop linear regression models for Above-Ground Biomass (AGB) and Dominant Height (DH) based on LiDAR metrics. Using stepwise variable selection, I optimize the model to improve predictive accuracy. I evaluate model performance by analyzing correlation matrices and comparing predicted values against measured data.\nOnce the best-fit models are identified, I apply them to LiDAR-derived rasters to generate spatial predictions of AGB and DH across the study area. By performing raster algebra, I compute differences between predicted and actual values, assessing the model’s effectiveness. Finally, I export the processed data and model outputs as .csv files for further analysis.\nThroughout this process, I use various visualization techniques, including plotting LiDAR-derived metrics, DEMs, CHMs, and model results. I also generate correlation matrices to examine relationships between different variables and validate the accuracy of my models.\nOverall, this report presents a complete LiDAR processing pipeline, from raw .las files to model-based forest attribute estimation. By integrating spatial data analysis, raster computation, and statistical modeling, I develop a data-driven approach to assessing forest structure and biomass distribution."
  },
  {
    "objectID": "lidr3dmodeling.html#load-in-necessary-packages",
    "href": "lidr3dmodeling.html#load-in-necessary-packages",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Load in necessary packages",
    "text": "Load in necessary packages\n\n\nShow/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(rgl)\nlibrary(htmlwidgets)"
  },
  {
    "objectID": "lidr3dmodeling.html#set-wd-and-load-in-some-data",
    "href": "lidr3dmodeling.html#set-wd-and-load-in-some-data",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Set WD and Load in some Data",
    "text": "Set WD and Load in some Data\n\n\nShow/Hide Code\n# set working directory\nsetwd(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\")\nwd &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\"   # create a variable string working directory\n\n# load in the RGB image and explore the structure\nrgb_afrf &lt;- rast(\"~/GEM 521/Lab 4/L4_Data/Aerial_Photo/AFRF_Aerial_Photo.tif\") # read in the .tif aerial photo using the rast function\nstr(rgb_afrf)         # display the structure of the SpatRaster\n\n\nS4 class 'SpatRaster' [package \"terra\"]\n\n\nShow/Hide Code\nplotRGB(rgb_afrf)     # plot the SpatRaster as an RGB image"
  },
  {
    "objectID": "lidr3dmodeling.html#set-working-directory",
    "href": "lidr3dmodeling.html#set-working-directory",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Set Working Directory",
    "text": "Set Working Directory\n\n\nShow/Hide Code\n# set working directory\nsetwd(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\")\nwd &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\"   # create a variable string working directory\n\n# load in the RGB image and explore the structure\nrgb_afrf &lt;- rast(\"~/GEM 521/Lab 4/L4_Data/Aerial_Photo/AFRF_Aerial_Photo.tif\") # read in the .tif aerial photo using the rast function\nstr(rgb_afrf)         # display the structure of the SpatRaster\nplotRGB(rgb_afrf)     # plot the SpatRaster as an RGB image\n\n\n\n\n\nAerial Image of the true color composite for Alex Fraser Research Forest"
  },
  {
    "objectID": "lidr3dmodeling.html#plot-2d-digital-elevation-model",
    "href": "lidr3dmodeling.html#plot-2d-digital-elevation-model",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Plot 2D Digital Elevation Model",
    "text": "Plot 2D Digital Elevation Model\n\n\nShow/Hide Code\n#Create LAScatalog object from afrf las tiles\ncat_afrf &lt;- readLAScatalog(\"~/GEM 521/Lab 4/L4_Data/LAS\")\n\n#Set the output directory for the filtered .las data\nopt_output_files(cat_afrf) &lt;- paste(wd, \"/Filtered/filtered_afrf_{ID}\", sep = \"\")\n\n#read filtered .las into LAScatalog\nfiltered_cat_afrf &lt;- readLAScatalog(\"~/GEM 521/Lab 4/L4_Data/Filtered\")\n\n#Create DEM\ndem_allLAS_afrf &lt;- rasterize_terrain(filtered_cat_afrf, 2, tin())\n\n#Create color palette\ncol_1 &lt;- height.colors(50) \n\n#Plot DEM using color palette\nplot(dem_allLAS_afrf, col = col_1,\n     main = 'DEM of Alex Fraser Research Forest') #plot in 2D\n\n\n\n\n\nDigital Elevation Model (DEM) of Alex Fraser Research Forest"
  },
  {
    "objectID": "lidr3dmodeling.html#create-and-plot-chm",
    "href": "lidr3dmodeling.html#create-and-plot-chm",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Create and Plot CHM",
    "text": "Create and Plot CHM\n\n\nShow/Hide Code\n#read normalized las into catalog to continue processing\nnorm_cat_afrf &lt;- readLAScatalog(\"~/GEM 521/Lab 4/L4_Data/Normalized\")\n\n#add LAScatalog enginge option to filter undersired data points\nopt_filter(norm_cat_afrf) &lt;- '-drop_z_below 0 -drop_z_above 55'\n\n#Create CHM for all normalized afrf Tiles\nchm_afrf &lt;- rasterize_canopy(norm_cat_afrf, 2, p2r()) \nplot(chm_afrf, col = col_1,\n     main = 'Canopy Height Model of Alex Fraser Research Forest') #plot in 2D\n\n\n\n\n\nCanopy Height Model (CHM) of Alex Fraser Research Forest"
  },
  {
    "objectID": "lidr3dmodeling.html#plot-extraction",
    "href": "lidr3dmodeling.html#plot-extraction",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Plot Extraction",
    "text": "Plot Extraction\nWe extract the field plots and calculate some cloud metrics\nThen combine the plots and cloud metrics into one data table.\n\n\nShow/Hide Code\n# read in the .csv table with field data\nafrf_plot_table &lt;- read.csv(\"~/GEM 521/Lab 4/L4_Data/Plots/Plot_Table.csv\")\n\n# define the size if the plot radius\nradius &lt;- 10\n\n#for loop to extract multiple plots\nfor(i in 1:nrow(afrf_plot_table)){ #run the loop until i = the number of rows in 'plot_table' (20)\n  plot_cent &lt;- c(afrf_plot_table$X[i], afrf_plot_table$Y[i]) #extract plot center\n  plot_las &lt;- clip_circle(norm_cat_afrf, plot_cent[1], plot_cent[2], radius) #clip plot from norm_cat_las\n  output_file &lt;- paste(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_\", i, \".las\", sep = \"\") #output directory as string\n  writeLAS(assign(paste(\"afrf_Plot_\", i, sep = \"\"), plot_las), output_file) #write'afrf_Plot_i' to output dir.\n}\n\n#check a plot\nplot1 &lt;- readLAS(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_1.las\")\nplot(plot1)\n\n#Calculate cloud metrics for all plots\n#create empty dataframe\nafrf_plot_metrics_2 &lt;- data.frame() \n\n#For loop to calculate cloud metrics for all plots and add them to 'afrf_cloud_metrics'\nfor(i in 1:nrow(afrf_plot_table)){ #for loop == number of rows in plot_table (20)\n  plot &lt;- readLAS(paste(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_\", i, \".las\", sep= \"\"), filter = '-drop_z_below 0 -drop_z_above 55')\n  metrics &lt;- cloud_metrics(plot, .stdmetrics) #compute standard metrics\n  afrf_plot_metrics_2 &lt;- rbind(afrf_plot_metrics_2, metrics) #add the new 'metrics' to 'afrf_cloud_metrics'\n}\n\n#Add column to \"afrf_plot_metrics' called Plot_ID (join key)\nafrf_plot_metrics_2$Plot_ID = 1:38 # create a column with values 1-20 to join the plot_table to based on the ID.\n\n#Join 'Plot_Table' and 'afrf_Plot_Metrics' into 'data_table'\ndata_table &lt;- afrf_plot_table %&gt;% \n  full_join(afrf_plot_metrics_2) # join the plot_table to afrf_plot in a seperate frame called data_table.\n\n\n\n\nShow/Hide Code\n#check a plot\nplot1 &lt;- readLAS(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_1.las\")\nplot(plot1)\n\n# Open a 3D rendering window for Li et al.2012\nopen3d()\nplot(plot1)\nrglwidget_obj_plot1 &lt;- rglwidget()\nsaveWidget(rglwidget_obj_plot1, file = \"interactive_plot1.html\", selfcontained = TRUE)"
  },
  {
    "objectID": "lidr3dmodeling.html#develop-statistical-model-for-agb-estimation",
    "href": "lidr3dmodeling.html#develop-statistical-model-for-agb-estimation",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Develop Statistical Model for AGB Estimation",
    "text": "Develop Statistical Model for AGB Estimation\n\n\nShow/Hide Code\nmodelAGB &lt;- lm(Total_AGB ~ zmean + zsd + zmax, data = data_table)\n\n# plot predicted model vs measured volume\nplot(Total_AGB ~ modelAGB$fitted, \n     data = data_table, \n     xlab = 'Predicted', \n     ylab = 'Measured',\n     main = 'ModelAGB Predicting AGB (kg/ha)')\nabline(0, 1) # fit a one to one line of best fit\n\n\n\n\n\nPlot of the predicted vs measured AGB using modelAGB\n\n\n\n\nShow/Hide Code\n# define the metrics we want\nf &lt;- function(z) {\n  list(\n    zmean = mean(z), \n    zsd = sd(z),\n    zmax = max(z))\n}\n\n#Calculate pixel metrics for entire study area\npixel_metrics_afrf &lt;- pixel_metrics(norm_cat_afrf, func = ~f(Z), 10)\n\n# subset zq95\nzmean_afrf &lt;- terra::subset(pixel_metrics_afrf, \"zmean\")\n\n# subset zsd\nzsd_afrf &lt;- terra::subset(pixel_metrics_afrf, \"zsd\")\n\n# subset zmax\nzmax_afrd &lt;- terra::subset(pixel_metrics_afrf, \"zmax\")\n\n# stack raster\nraster_stack_AGB &lt;- c(zmean_afrf, zsd_afrf, zmax_afrd)\n\n# create function from model\nAGB_func &lt;- function(x){\n  30539.090*x[1] - 25978.149*x[2] + 5336.517*x[3] - 44006.100\n}\n\n# apply function to raster\nmodel_AGB_predictor &lt;- terra::app(raster_stack_AGB, AGB_func)\nplot(model_AGB_predictor,\n     main = 'Estimated AGB using ModelAGB (kg/ha)')\n\n\n\n\n\nFinal AGB Predictions using modelAGB"
  },
  {
    "objectID": "itd.html",
    "href": "itd.html",
    "title": "Individual Tree Detection Methods",
    "section": "",
    "text": "In this report, I preprocess and analyze LiDAR data using R and various geospatial libraries in R such as lidR, terra, and tidyverse. The objective is to detect trees, individually, by using two different algorithms.\nThe (Li et al. 2012) method relies on a local maxima filtering approach applied to a Canopy Height Model (CHM). I smooth the CHM to reduce noise and extract tree tops using a variable window size, ensuring better accuracy in detecting dominant trees and visualize the detected trees and assess the segmentation results.\nThe (Dalponte and Coomes 2016) method incorporates a marker-controlled watershed segmentation, which refines tree crown delineation based on spectral and structural characteristics. Using a Gaussian smoothing filter and use the tree segmentation function in lidR to identify tree crowns, I compare the results with the Li et al. method to evaluate their effectiveness. Throughout this process, I fine-tune parameters such as smoothing intensity, window size, and threshold values to optimize tree detection accuracy. The results allow me to assess the effectiveness of these ITD methods in forest structure analysis and remote sensing applications."
  },
  {
    "objectID": "itd.html#load-packages",
    "href": "itd.html#load-packages",
    "title": "Individual Tree Detection Methods",
    "section": "Load Packages",
    "text": "Load Packages\n\n\nShow/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(rgl)\nlibrary(htmlwidgets)\n\n# set working directory\nsetwd(\"~/GEM 521/Lab 5/Data\")\nwd &lt;- \"~/GEM 521/Lab 5/Data\"   # create a variable string working directory"
  },
  {
    "objectID": "itd.html#create-dem",
    "href": "itd.html#create-dem",
    "title": "Individual Tree Detection Methods",
    "section": "Create DEM",
    "text": "Create DEM\nThe first thing we need to produce is a Digital Elevation Model. This is so we can normalize the LiDAR data to an accurate elevation so we can extract just the points we need to create our models.\n\n\nShow/Hide Code\n# read filtered .las into LAScatalog\nfiltered_cat_mkrf &lt;- readLAScatalog(\"~/GEM 521/Lab 5/Data/Filtered\")\nsummary(filtered_cat_mkrf)\nplot(filtered_cat_mkrf)\n\n# create color palette\ncol_1 &lt;- height.colors(50)\n\n# create DEM\ndem_allLAS_mkrf &lt;- rasterize_terrain(filtered_cat_mkrf, 2, tin())\n\n# plot DEM using color palette\nplot(dem_allLAS_mkrf, col = col_1,\n     main = 'DEM of Alex Fraser Research Forest') #plot in 2D\n\n\n\n\n\nDEM of Malcom Knapp Research Forest"
  },
  {
    "objectID": "itd.html#create-chm",
    "href": "itd.html#create-chm",
    "title": "Individual Tree Detection Methods",
    "section": "Create CHM",
    "text": "Create CHM\nAfter we have the DEM, we can normalize the LAS catalog to the DEM we just created. Once normalize, we can then rasterize the LAS in order to plot the Canopy Height Model (CHM). This is a model that shows the height, or tops, of the trees.\n\n\nShow/Hide Code\n# read normalized las into catalog to continue processing\nnorm_cat_mkrf &lt;- readLAScatalog(\"~/GEM 521/Lab 5/Data/Normalized\")\n\n# add LAScatalog enginge option to filter undersired data points\nopt_filter(norm_cat_mkrf) &lt;- '-drop_z_below 0 -drop_z_above 65'\n\n# ensure the entire study area was processed\nplot(norm_cat_mkrf)\nsummary(norm_cat_mkrf)\n\n# plot the CHM just to make sure things look good\nplot(chm_mkrf, col = col_1,\n     main = 'Canopy Height Model of Alex Fraser Research Forest') # plot in 2D\n\n\n\n\n\nCanopy Height Model of Malcom Knapp Research Forest"
  },
  {
    "objectID": "itd.html#extract-plots",
    "href": "itd.html#extract-plots",
    "title": "Individual Tree Detection Methods",
    "section": "Extract Plots",
    "text": "Extract Plots\nNow that we have our normalized CHM of the study site, we can start to work on the individual tree detection. First we need to access the field data where the plot data was recorded. There were four plots that were extracted, with a radius of 154 meters that were extracted, using given coordinates based on the .csv. After extraction, we can read in the plots as LAS files to compute metrics as well as visualize them. We can see one of the plots visualized in 3D below."
  },
  {
    "objectID": "itd.html#visualize",
    "href": "itd.html#visualize",
    "title": "Individual Tree Detection Methods",
    "section": "Visualize",
    "text": "Visualize\nBelow is a interactive visualization of one of the extracted plots from Malcom Knapp Research Forest. We will be detecting each one of these trees using two different tree detection methods."
  },
  {
    "objectID": "itd.html#itd-using-li-2012",
    "href": "itd.html#itd-using-li-2012",
    "title": "Individual Tree Detection Methods",
    "section": "ITD using Li (2012)",
    "text": "ITD using Li (2012)"
  },
  {
    "objectID": "itd.html#itd-using-dalponte-coomes-2016",
    "href": "itd.html#itd-using-dalponte-coomes-2016",
    "title": "Individual Tree Detection Methods",
    "section": "ITD using Dalponte Coomes (2016)",
    "text": "ITD using Dalponte Coomes (2016)"
  },
  {
    "objectID": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies",
    "href": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv2.html#publications",
    "href": "cv2.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "cv2.html#skills-and-certificates",
    "href": "cv2.html#skills-and-certificates",
    "title": "CV",
    "section": "Skills and Certificates",
    "text": "Skills and Certificates\n\nGIS and Spatial Data Management\n\nArcGIS Pro, QGIS, ENVI, FME\nArcGIS Model Builder and Experience Builder\nPostgreSQL for Spatial Databases\nRemote Sensing & Cartography\n\n\n\nTechnical & Software Procifiency\n\nPython, R, SQL\nAdobe Suite (Illustrator, Photoshop), SketchUp, Procreate\nMicrosoft Office (Excel, Word, PowerPoint)\n\n\n\nCertifications & Licenses\n\nClass 5 Drivers License\nWorkSafeBC\nWHMIS Certified\nFirst Aid (WFA 40 Hour)\nBoating License\nDrone License"
  },
  {
    "objectID": "cv2.html#education",
    "href": "cv2.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\n\nMasters in Geomatics for Environmental Management UBC Vancouver (2024-2025)\nB.Sc Earth and Environmental Science UBC Okanagan (2016-2019)"
  },
  {
    "objectID": "cv2.html#professional-experience",
    "href": "cv2.html#professional-experience",
    "title": "CV",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nLead Field Technician at AquaCoustic Remote Technologies\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software\n\n\n\n\nCo-Founder Canopy Campers Van Rentals\n(2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking\n\n\n\nVisitors Service Attendant for Banff National Park\n(Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively\n\n\n\nPublications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "burnseverity2.html",
    "href": "burnseverity2.html",
    "title": "Burn Severity Analysis of Prouton Lakes Fire",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#Attach packages\n\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyr)\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\", quiet = TRUE)\n\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  #sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  #ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  #nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  #writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  #writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  #writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\npre_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\")\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Pre-Fire (2015)\")\n\n\n\n\n\n\n\n\n\nCode\n# post-fire is July 15 2018\npost_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\")\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Post-Fire (2018)\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\n\n\n\n\n\n\n\nCode\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "murrelet.html",
    "href": "murrelet.html",
    "title": "Marbled Murrelet Habitat Connectivity",
    "section": "",
    "text": "Introduction\nThis study aims to investigate the relationship between nesting habitats of the Marbled Murrelet and mapped sensitive ecosystems on Gambier Island in British Columbia. Using a patch-based connectivity model evaluating habitat availability called Grains of Connectivity (GoC), the research will focus on the critical habitat of the Marbled Murrelet and their spatial relation to existing Old Growth Management Areas (OGMA). This study aims to provide insight on the habitat fragmentation and the spatial connectivity in hopes to provide leverage for effectively conserving this critical species at risk and fragile ecosystems.\nGambier Island, also known as Cha7élkwnech in the Squamish language, is located in the Howe Sound near Vancouver, BC. Encompassing roughly 70 square kilometers of complex landscapes and sensitive ecosystems (Figure 1), the present forests serve as important habitats for a range of endangered species (Butler et al., n.d.). The historical pressures from logging, human settlement, and recreation have affected the biodiversity of the landscape and requires vigilant assessment to maintain a healthy ecosystem.\nThe Marbled Murrelet is a threatened seabird that has the potential to be strongly influenced by forest fragmentation and edge effects (Malt and Lank 2009). They spend most of their time searching for food in coastal waters, but unlike other seabirds, they can fly up to 50km inland to nest (Nelson 2020) and are strongly associated with late-successional and old-growth forests, buildings its’ nests on large branches in tall trees (Raphael, Mack, and Cooper 2002). The species was listed under the Species at Risk Act in 2012 and has been identified as a priority for conservation in many regional conservation strategies in Canada (Bertram et al. 2015).\n\n\nMethods\nThe data for this study comes from two primary data sources. The first source is the Island Trust, a special purpose government serving the islands in the Salish Sea and Howe Sounds responsible for preserving and protecting the islands’ environments (Forests, n.d.). The second source is the Critical Habitat for Species at Risk National Dataset of Canada created by Environment and Climate Change Canada (ECCC) for terrestrial species (Canada and Change, n.d.). These two data sets will be analyzed using R and ArcGIS Pro software and serve for the ecological network connectivity analysis.\nThe first step in this analysis is to rasterize the Sensitive Ecosystem Mapping (SEM) Air photo such that it can be used to create a land classification for the connectivity analysis. The second step is to do a similar rasterization of the Marbled Murrelet data and attribute link threshold characteristics to the seabird data set like flight distance capabilities and preferential flight pathing. Similarly, resistance values need to be assigned to the terrestrial classified landscape in order to create a minimum planar graph (MPG). These resistance values will affect the movement of Marbled Murrelet through the landscape and are necessary to understand the species connectivity and reliance on specific ecosystems.\n\n\n\nGambier Island Sensitive Ecosystem Mapping for Gambier Island\n\n\nAfter the rasterization and landscape reclassification of the SEM, OGMA and Marbled Murrelet habitat, a series of ecological analyses will be performed. Specifically, a network analysis utilizing the MPG to create a least-cost path model for the Marbled Murrelet, and with varying threshold values, a grains of connectivity (GoC) analysis. A variety of landscape metrics using the landscapemetrics package in R will be applied to the reclassified ecosystems including PLAND (percentage of landscape), NP (number of patches), CAI_MN (mean of core area index), LPI (largest patch index) and other metrics useful to determine the fragmentation and ecosystem edges.\n\n\n\n\n\n\n\n\nFigure 1: National Landcover Data for Howe Sound, Based on August 2023 data set.\n\n\n\n\n\nOnce we have our classified landscape, we can use the landscape metrics to extract the core areas and patches using specific landscape_metrics functions. Below is the resulting core areas of the Howe Sound landscape.\n Now that we have our core areas, lets compare structural connectivity between barren land, evergreen forests, and low developed classes.\nWe’ll compare the following metrics at the class-level:\n\nCAI_MN: Mean of core area index (Core area metric)\nCOHESION: Patch Cohesion Index (Aggregation metric)\nLPI: Largest patch index (Area and Edge metric)\nNP: Number of patches (Aggregation metric)\nPLAND: Percentage of landscape of class (Area and Edge metric)\n\n\n\n\n\n\n\n\n\nFigure 2: Values for structural connectivity metrics for the current landscape.\n\n\n\n\n\n\n\nExpected Results & Significance\nThis study is expected to show a connection between the preferred nesting habitats of the Marbled Murrelet, where they are located in relation to the mapped ecosystems on Gambier Island, and their proximity to the OGMAs. The analysis will likely reveal that edge forests, old growths, and other mature forested areas on the Island correlate to the birds’ nesting habitat, in other words, provide crucial habitat sites for the species. There will likely be a connection between the extent of forested edges and its influence on the nesting habitats as the Murrelet.\nThe connectivity analysis will highlight the links between the critical habitats and classified ecosystems, identifying preferential pathways for the Murrelet as well as barriers to their movement. It is likely that the values of calculated metrics will confirm the fragmentated state of the bird’s habitat and current OGMAs, supporting the need for further improvement of land protection and conservation efforts. It will link the locality and fragmentation of the OGMAs to Gambier’s SEM and Murrelet habitat. Ultimately, by identifying critical connections and areas of fragmentation, this study will inform future efforts to reduce habitat loss and improve landscape connectivity for species conservation on Gambier Island.\n\n\n\n\n\nReferences\n\nBertram, Douglas F., Mark C. Drever, Murdoch K. McAllister, Bernard K. Schroeder, David J. Lindsay, and Deborah A. Faust. 2015. “Estimation of Coast-Wide Population Trends of Marbled Murrelets in Canada Using a Bayesian Hierarchical Model.” PLOS ONE 10 (8): e0134891. https://doi.org/10.1371/journal.pone.0134891.\n\n\nButler, Robert W, Rod MacVicar, Andrew R Couturier, Sonya Richmond, Eva Dickson, and Patricia Beaty. n.d. “Status and Distribution of Marine Birds and Mammals in Southern Howe Sound, British Columbia.”\n\n\nCanada, Environment, and Climate Change. n.d. “Critical Habitat for Species at Risk National Dataset - Canada - Open Government Portal.” https://open.canada.ca/data/dataset/47caa405-be2b-4e9e-8f53-c478ade2ca74.\n\n\nForests, Ministry of. n.d. “Forest Stewardship - Province of British Columbia.” https://www2.gov.bc.ca/gov/content/industry/forestry/managing-our-forest-resources.\n\n\nMalt, Joshua M., and David B. Lank. 2009. “Marbled Murrelet Nest Predation Risk in Managed Forest Landscapes: Dynamic Fragmentation Effects at Multiple Scales.” Ecological Applications 19 (5): 1274–87. https://doi.org/10.1890/08-0598.1.\n\n\nNelson, S. Kim. 2020. “Marbled Murrelet (Brachyramphus Marmoratus), Version 1.0.” Birds of the World. https://doi.org/10.2173/bow.marmur.01species_shared.bow.project_name.\n\n\nRaphael, Martin G., Diane Evans Mack, and Brian A. Cooper. 2002. “Landscape-Scale Relationships Between Abundance of Marbled Murrelets and Distribution of Nesting Habitat.” The Condor 104 (2): 331–42. https://doi.org/10.1093/condor/104.2.331."
  },
  {
    "objectID": "vanlife.html",
    "href": "vanlife.html",
    "title": "Off Grid Camper Van Build",
    "section": "",
    "text": "After my undergrad at UBC Okanagan in 2019, I bought a 2005 Mercedes Sprinter with the dream of converting it into a full time camper van. I wanted to use this to travel along the Pacific West Coast. I had recently gained a strong interest in outdoor rock climbing, so the plan was to build out this van with the basic bells and whistles (I only had a month) before I started my venture south towards Los Angeles where my brother was at the time.\nI gained some valuable handy-work skills from my father (an engineer who spend a lot of time working on projects around the house) and, with the help of a lot of YouTube videos and the occasional blog post, I felt I had the skills to build a fully functional, off-grid camper van.\nBelow are some snapshots along the building process.\nThis page is currently a work in progress.\n\n\n\nMe inside the van installing the insulation\n\n\n\n\n\nMe on top of the van cutting out a hole for the fan\n\n\n\n\n\nThe countertop during installation\n\n\n\n\n\nInstalling Solar Panels on the Roof\n\n\n\n\n\nTongue and Groove Ceilings\n\n\n\n\n\nSlide out drawers under the slat slide-out bed/couch\n\n\n\n\n\nThe Van in all her beauty\n\n\nIts 2025 and I still own the van! I use it daily from little commutes (not exactly the most convenient) to big road trips (extremely convenient). I cannot think of a better vehicle for adventures, something that I seek often, including hiking, skiing, camping, climbing, and other back country adventures!\nMore content coming soon!"
  },
  {
    "objectID": "vanlife.html#page-currently-under-construction",
    "href": "vanlife.html#page-currently-under-construction",
    "title": "Off Grid Camper Van Build",
    "section": "",
    "text": "After my undergrad at UBC Okanagan in 2019, I bought a 2005 Mercedes Sprinter with the dream of converting it into a full time camper van. I wanted to use this to travel along the Pacific West Coast. I had recently gained a strong interest in outdoor rock climbing, so the plan was to build out this van with the basic bells and whistles (I only had a month) before I started my venture south towards Los Angeles where my brother was at the time.\nI gained some valuable handy-work skills from my father (an engineer who spend a lot of time working on projects around the house) and, with the help of a lot of YouTube videos and the occasional blog post, I felt I had the skills to build a fully functional, off-grid camper van.\nBelow are some snapshots along the building process.\nThis page is currently a work in progress.\n\n\n\nMe inside the van installing the insulation\n\n\n\n\n\nMe on top of the van cutting out a hole for the fan\n\n\n\n\n\nThe countertop during installation\n\n\n\n\n\nInstalling Solar Panels on the Roof\n\n\n\n\n\nTongue and Groove Ceilings\n\n\n\n\n\nSlide out drawers under the slat slide-out bed/couch\n\n\n\n\n\nThe Van in all her beauty\n\n\nIts 2025 and I still own the van! I use it daily from little commutes (not exactly the most convenient) to big road trips (extremely convenient). I cannot think of a better vehicle for adventures, something that I seek often, including hiking, skiing, camping, climbing, and other back country adventures!\nMore content coming soon!"
  },
  {
    "objectID": "otherprojects.html",
    "href": "otherprojects.html",
    "title": "Other Projects",
    "section": "",
    "text": "In 2021, during COVID19 lock down, I undertook a project for a family friend in the design and construction of a floating staircase made of Jatoba hardwood.\nThe designing was done on SketchUp, and all the woodworking in a nearby workshop.\nBelow are some images of this project.\n\n\n\nThe stairwell, gutted and ready for a new staircase!\n\n\n\n\n\nModelling the staircase using SketchUp\n\n\n\n\n\nAnother screeshot of the modelling process\n\n\n\n\n\nMany hours later, the almost finished project (the handrails were not documented)\n\n\n\n\n\nAnother shot of the finished product\n\n\n\n\n\nThe overall shot. The client was extremely pleased."
  },
  {
    "objectID": "otherprojects.html#page-currently-under-construction",
    "href": "otherprojects.html#page-currently-under-construction",
    "title": "Other Projects",
    "section": "",
    "text": "In 2021, during COVID19 lock down, I undertook a project for a family friend in the design and construction of a floating staircase made of Jatoba hardwood.\nThe designing was done on SketchUp, and all the woodworking in a nearby workshop.\nBelow are some images of this project.\n\n\n\nThe stairwell, gutted and ready for a new staircase!\n\n\n\n\n\nModelling the staircase using SketchUp\n\n\n\n\n\nAnother screeshot of the modelling process\n\n\n\n\n\nMany hours later, the almost finished project (the handrails were not documented)\n\n\n\n\n\nAnother shot of the finished product\n\n\n\n\n\nThe overall shot. The client was extremely pleased."
  },
  {
    "objectID": "canopycampers.html",
    "href": "canopycampers.html",
    "title": "Canopy Campers",
    "section": "",
    "text": "In 2023, two friends and I decided to purchase 3 vehicles (one each) with the idea of creating a small scale business that rents out vans for those seeking to explore the beautiful British Columbia and surrounding areas. Using my experience with building vans, I took on the interior build out task, which involved designing and building modular and functional mini van build outs. Since all the vehicles were of different models, each build was unique and completely costume.\nDuring my time building out and renting these vehicles, I gained valuable experience with:\n\nModelling of van interiors using SketchUp to design fully functional, camper vans that sleep and seat up to 5 guests\nCustom woodworking: slide out dinette with sink, stove, cooler and storage as well as a full-sized bed\nCustomer interactions including responding to inquiries, showing clients how to use features of the campers and key hand-offs\nVehicle maintenance and repairs, regular cleaning, organizing, and stocking of vehicles\n\nBelow are a few images of those vehicles and their builds.\n!"
  },
  {
    "objectID": "canopycampers.html#page-under-construction",
    "href": "canopycampers.html#page-under-construction",
    "title": "Canopy Campers",
    "section": "",
    "text": "In 2023, two friends and I decided to purchase 3 vehicles (one each) with the idea of creating a small scale business that rents out vans for those seeking to explore the beautiful British Columbia and surrounding areas. Using my experience with building vans, I took on the interior build out task, which involved designing and building modular and functional mini van build outs. Since all the vehicles were of different models, each build was unique and completely costume.\nDuring my time building out and renting these vehicles, I gained valuable experience with:\n\nModelling of van interiors using SketchUp to design fully functional, camper vans that sleep and seat up to 5 guests\nCustom woodworking: slide out dinette with sink, stove, cooler and storage as well as a full-sized bed\nCustomer interactions including responding to inquiries, showing clients how to use features of the campers and key hand-offs\nVehicle maintenance and repairs, regular cleaning, organizing, and stocking of vehicles\n\nBelow are a few images of those vehicles and their builds.\n!"
  },
  {
    "objectID": "cortes.html",
    "href": "cortes.html",
    "title": "This is Cortes",
    "section": "",
    "text": "Authors: Genevieve Doiron & Max Lorsignol\n\n\nCortes Island is home to approximately 1000 year-round residents dispersed between the communities of Cortes Bay, Whaletown, Squirrel Cove and Mansons Landing (“Our Community” 2013). The community is diverse both in age and background. Including working professionals, young families, and retirees. Professions on the Island include “farmers, loggers, artists, educators, retirees” and those seeking a slower pace of life (“Our Community” 2013). Their core values include independence, community connection, economic security, and preservation of nature. The communities on the island are close-knit, with active participation in community events and initiatives by locals of all ages (“Our Community” 2013). They are connected by their shared desire to live independently, yet collaboratively, in a rural and natural setting. The values of self-reliance and resource based livelihoods are echoed in many of their chosen professions as farmers, loggers, and crafts-people. The communities on the island are quite rural, much of the appeal to live there is the area’s rugged wilderness and natural beauty, which also underpins the island’s tourism industry. With regards to the Cortes Forestry General Partnership community members value the forest through their personal connection to nature as well as their reliance on the forest and other natural resources for their livelihoods (Bullock and Hanna 2012). Messaging should appeal to ethos and pathos; ethos through the delivery of the message from trusted and respected community members and pathos through an appeal to their emotional connection to the land and their community.\n\n\n\n\n\n\nDigital Art (by Max Lorsignol) representing the Local, Non-Indigenous People of Cortes\n\n\nRenewing the forest agreement is essential to ensuring that the voices of Cortes residents remain at the heart of decisions that shape our future. This agreement is crafted by us, for us—those who live here, understand this land, and are committed to its well-being. Outsiders should not determine the future of our island; we know our community and what it needs to thrive. This agreement protects our right to support each other and secure a sustainable future. It will bring us jobs, strengthening our local economy, and safeguard the forest, ensuring its health for generations to come. Together, we can shape the future of Cortes, protecting both our people and our environment.\nThe core message of renewing the forest agreement directly appeals to the values of independence, community, livelihoods, and connection to nature that are central to Cortes Island residents. By centering that the agreement ensures decisions about the future of the island are made by those who live there, the message upholds the community’s desire for self-reliance and local control. It also highlights the economic benefits of sustainable forestry, promising job creation and a thriving local economy without compromising the island’s natural resources.\nThe message appeals to the strong emotional bond residents have with the island’s wilderness. It positions the renewed agreement as a way to safeguard the forest for future generations, resonating with their commitment to preserving the environment as well as the multi-generational nature of the community. It offers a vision of ecological sustainability, where the forest is preserved not only for the current generation but for their children and grandchildren as well. Cortes Island is a close-knit community, where collaboration and mutual support are integral to daily life. By emphasizing that the agreement is crafted by residents, for residents, the message highlights the role of the community in shaping its own future. It reminds residents that, together, they can make decisions that benefit both people and the land, fostering a sense of unity and collective responsibility. It encourages residents to unite in protecting both the land and their way of life, ensuring that Cortes remains the vibrant community they love.\nThe message focuses on community more than the economic benefits of the partnership because people tend to respond most strongly to appeals on emotion. Words such as “us”, “together”, “heart”, and “protect” were used to invoke an emotional response towards security and closeness, such that the residents would opt to stay with the status quo. This message would be best delivered by trusted and respected community members, appealing to ethos and pathos at once. The visual is meant to support the message by depicting the beautiful nature they love with the vibrant diverse and inter-generational community it supports. The forest is drawn as lush and vibrant, framing the resource as the center of the discussion and alluding to the fact that supporting the agreement would support the forest health.\n\n\n\nThe British Columbia Ministry of Forests is committed to a sustainable and competitive forest sector that prioritizes forest stewardship, worker support, innovation, and climate resilience. The Ministry values collaboration with First Nations, industry, and other stakeholders to ensure that forest management practices are effective, inclusive, and forward-thinking (Forests, n.d.). Generally their framework is guided by western ideologies, scientific research and economic priorities. While they publicize their commitment to integrate Indigenous knowledge systems within that framework, historically the approach is of a western, empirical and policy driven perspective. That being said, as a government body, the Ministry is also deeply concerned with public perception and voter approval. As such, they would likely respond to messaging that aligns itself with their public priorities. They would be seeking to demonstrate that the forest policies they promote benefit both the environment and local communities.\nMessaging to the Ministry should appeal to logos, emphasizing the proven track record of the CFGP in successfully balancing environmental sustainability, local job creation, and community collaboration. Governments are typically resistant to change, especially when existing solutions are working, so highlighting the cooperative’s successful model will help reinforce the idea that this approach is effective, trusted, and aligned with the Ministry’s goals for sustainability and public support.\n\n\n\n\n\n\nDigital Art (by Max Lorsignol) representing the Minitstry of Forests, forward thinking approach\n\n\nThe Cortes Forestry General Partnership (CFGC) has proven success; it is economically viable, environmentally responsible, and widely supported. Through four community-backed harvests, the CFGC has shown the effectiveness of a public, private, resident, and Indigenous cooperation. The partnership has strengthened the local economy, created jobs, and preserved the island’s natural resources. The best path forward is one with a proven foundation, built brick by brick by the community it supports. The best path forward is together.\nThe core message directly appeals to the Ministry of Forests’ focus on sustainability, economic viability, and public support. The message is grounded in logos, highlighting the proven success of CFGP in balancing environmental stewardship, job creation, and community collaboration. The message appeals to the government’s goal to create a competitive forest sector by referring to the successful harvests and jobs created. It also appeals to ethos by stating that the forestry practices of the agreement are scientifically grounded.\nAs well, the emphasis on community support is particularly relevant to the Ministry, as public perception and voter approval are critical factors in government decision-making. The successful integration of local residents and the Klahoose First Nation through the CFGP showcases a collaborative approach that the government seeks to foster. As well given past disagreements and protests from the First Nation and non-Indigenous community members with forestry companies (Bullock and Hanna 2012), highlighting the community support of the agreement also positions it as superior to previous approaches. The visual created supports this message by showing government officials, community members, and First Nations leaders collaborating in a round table discussion. The visual depicts this happening in the forest, which is drawn as lush and vibrant with towering trees. This is in order to center the resource and frame it as prosperous and healthy while shown in tandem with heavy machinery – depicting a sustainable forest industry. Government’s are typically risk averse and resistant to change, especially when a plan is working effectively. The message directly plays to this understanding by stating the agreement’s success and questioning any reason for change. The message clearly positions renewing the agreement as the proven, popular, and politically advantageous option."
  },
  {
    "objectID": "cortes.html#audience-specific-messaging-for-cortes-island",
    "href": "cortes.html#audience-specific-messaging-for-cortes-island",
    "title": "This is Cortes",
    "section": "",
    "text": "Authors: Genevieve Doiron & Max Lorsignol\n\n\nCortes Island is home to approximately 1000 year-round residents dispersed between the communities of Cortes Bay, Whaletown, Squirrel Cove and Mansons Landing (“Our Community” 2013). The community is diverse both in age and background. Including working professionals, young families, and retirees. Professions on the Island include “farmers, loggers, artists, educators, retirees” and those seeking a slower pace of life (“Our Community” 2013). Their core values include independence, community connection, economic security, and preservation of nature. The communities on the island are close-knit, with active participation in community events and initiatives by locals of all ages (“Our Community” 2013). They are connected by their shared desire to live independently, yet collaboratively, in a rural and natural setting. The values of self-reliance and resource based livelihoods are echoed in many of their chosen professions as farmers, loggers, and crafts-people. The communities on the island are quite rural, much of the appeal to live there is the area’s rugged wilderness and natural beauty, which also underpins the island’s tourism industry. With regards to the Cortes Forestry General Partnership community members value the forest through their personal connection to nature as well as their reliance on the forest and other natural resources for their livelihoods (Bullock and Hanna 2012). Messaging should appeal to ethos and pathos; ethos through the delivery of the message from trusted and respected community members and pathos through an appeal to their emotional connection to the land and their community.\n\n\n\n\n\n\nDigital Art (by Max Lorsignol) representing the Local, Non-Indigenous People of Cortes\n\n\nRenewing the forest agreement is essential to ensuring that the voices of Cortes residents remain at the heart of decisions that shape our future. This agreement is crafted by us, for us—those who live here, understand this land, and are committed to its well-being. Outsiders should not determine the future of our island; we know our community and what it needs to thrive. This agreement protects our right to support each other and secure a sustainable future. It will bring us jobs, strengthening our local economy, and safeguard the forest, ensuring its health for generations to come. Together, we can shape the future of Cortes, protecting both our people and our environment.\nThe core message of renewing the forest agreement directly appeals to the values of independence, community, livelihoods, and connection to nature that are central to Cortes Island residents. By centering that the agreement ensures decisions about the future of the island are made by those who live there, the message upholds the community’s desire for self-reliance and local control. It also highlights the economic benefits of sustainable forestry, promising job creation and a thriving local economy without compromising the island’s natural resources.\nThe message appeals to the strong emotional bond residents have with the island’s wilderness. It positions the renewed agreement as a way to safeguard the forest for future generations, resonating with their commitment to preserving the environment as well as the multi-generational nature of the community. It offers a vision of ecological sustainability, where the forest is preserved not only for the current generation but for their children and grandchildren as well. Cortes Island is a close-knit community, where collaboration and mutual support are integral to daily life. By emphasizing that the agreement is crafted by residents, for residents, the message highlights the role of the community in shaping its own future. It reminds residents that, together, they can make decisions that benefit both people and the land, fostering a sense of unity and collective responsibility. It encourages residents to unite in protecting both the land and their way of life, ensuring that Cortes remains the vibrant community they love.\nThe message focuses on community more than the economic benefits of the partnership because people tend to respond most strongly to appeals on emotion. Words such as “us”, “together”, “heart”, and “protect” were used to invoke an emotional response towards security and closeness, such that the residents would opt to stay with the status quo. This message would be best delivered by trusted and respected community members, appealing to ethos and pathos at once. The visual is meant to support the message by depicting the beautiful nature they love with the vibrant diverse and inter-generational community it supports. The forest is drawn as lush and vibrant, framing the resource as the center of the discussion and alluding to the fact that supporting the agreement would support the forest health.\n\n\n\nThe British Columbia Ministry of Forests is committed to a sustainable and competitive forest sector that prioritizes forest stewardship, worker support, innovation, and climate resilience. The Ministry values collaboration with First Nations, industry, and other stakeholders to ensure that forest management practices are effective, inclusive, and forward-thinking (Forests, n.d.). Generally their framework is guided by western ideologies, scientific research and economic priorities. While they publicize their commitment to integrate Indigenous knowledge systems within that framework, historically the approach is of a western, empirical and policy driven perspective. That being said, as a government body, the Ministry is also deeply concerned with public perception and voter approval. As such, they would likely respond to messaging that aligns itself with their public priorities. They would be seeking to demonstrate that the forest policies they promote benefit both the environment and local communities.\nMessaging to the Ministry should appeal to logos, emphasizing the proven track record of the CFGP in successfully balancing environmental sustainability, local job creation, and community collaboration. Governments are typically resistant to change, especially when existing solutions are working, so highlighting the cooperative’s successful model will help reinforce the idea that this approach is effective, trusted, and aligned with the Ministry’s goals for sustainability and public support.\n\n\n\n\n\n\nDigital Art (by Max Lorsignol) representing the Minitstry of Forests, forward thinking approach\n\n\nThe Cortes Forestry General Partnership (CFGC) has proven success; it is economically viable, environmentally responsible, and widely supported. Through four community-backed harvests, the CFGC has shown the effectiveness of a public, private, resident, and Indigenous cooperation. The partnership has strengthened the local economy, created jobs, and preserved the island’s natural resources. The best path forward is one with a proven foundation, built brick by brick by the community it supports. The best path forward is together.\nThe core message directly appeals to the Ministry of Forests’ focus on sustainability, economic viability, and public support. The message is grounded in logos, highlighting the proven success of CFGP in balancing environmental stewardship, job creation, and community collaboration. The message appeals to the government’s goal to create a competitive forest sector by referring to the successful harvests and jobs created. It also appeals to ethos by stating that the forestry practices of the agreement are scientifically grounded.\nAs well, the emphasis on community support is particularly relevant to the Ministry, as public perception and voter approval are critical factors in government decision-making. The successful integration of local residents and the Klahoose First Nation through the CFGP showcases a collaborative approach that the government seeks to foster. As well given past disagreements and protests from the First Nation and non-Indigenous community members with forestry companies (Bullock and Hanna 2012), highlighting the community support of the agreement also positions it as superior to previous approaches. The visual created supports this message by showing government officials, community members, and First Nations leaders collaborating in a round table discussion. The visual depicts this happening in the forest, which is drawn as lush and vibrant with towering trees. This is in order to center the resource and frame it as prosperous and healthy while shown in tandem with heavy machinery – depicting a sustainable forest industry. Government’s are typically risk averse and resistant to change, especially when a plan is working effectively. The message directly plays to this understanding by stating the agreement’s success and questioning any reason for change. The message clearly positions renewing the agreement as the proven, popular, and politically advantageous option."
  },
  {
    "objectID": "floatingstaircase.html",
    "href": "floatingstaircase.html",
    "title": "Floating Staircase",
    "section": "",
    "text": "In 2021, during COVID19 lock down, I undertook a project for a family friend in the design and construction of a floating staircase made of Jatoba hardwood.\nThe designing was done on SketchUp, and all the woodworking in a nearby workshop.\nBelow are some images of this project.\n\n\n\nThe stairwell, gutted and ready for a new staircase!\n\n\n\n\n\nModelling the staircase using SketchUp\n\n\n\n\n\nAnother screeshot of the modelling process\n\n\n\n\n\nMany hours later, the almost finished project (the handrails were not documented)\n\n\n\n\n\nAnother shot of the finished product\n\n\n\n\n\nThe overall shot. The client was extremely pleased."
  },
  {
    "objectID": "floatingstaircase.html#page-currently-under-construction",
    "href": "floatingstaircase.html#page-currently-under-construction",
    "title": "Floating Staircase",
    "section": "",
    "text": "In 2021, during COVID19 lock down, I undertook a project for a family friend in the design and construction of a floating staircase made of Jatoba hardwood.\nThe designing was done on SketchUp, and all the woodworking in a nearby workshop.\nBelow are some images of this project.\n\n\n\nThe stairwell, gutted and ready for a new staircase!\n\n\n\n\n\nModelling the staircase using SketchUp\n\n\n\n\n\nAnother screeshot of the modelling process\n\n\n\n\n\nMany hours later, the almost finished project (the handrails were not documented)\n\n\n\n\n\nAnother shot of the finished product\n\n\n\n\n\nThe overall shot. The client was extremely pleased."
  }
]